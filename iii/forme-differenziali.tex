\chapter{Forme differenziali lineari}
\section{Insiemi connessi}
Prima di introdurre la teoria delle forme differenziali, introduciamo la definizione di insieme \emph{connesso}.
\begin{definizione} \label{d:connesso}
	Un insieme $A\subset\R^n$ aperto si dice connesso se, laddove può essere scritto come unione di due insiemi aperti e disgiunti, uno dei due è vuoto, vale a dire
	\begin{equation*}
		A=A_1\cup A_2\text{ e }A_1\cap A_2=\emptyset\text{, allora }A_1=\emptyset\text{ o }A_2=\emptyset.
	\end{equation*}
\end{definizione}
Un'altra definizione si ottiene se consideriamo gli archi che collegano due punti dell'insieme: se per qualsiasi coppia di punti l'arco è contenuto tutto nell'insieme, anche in questo caso l'insieme si dice connesso
Il seguente teorema mostra che queste due definizioni sono in realtà interscambiabili.
\begin{teorema} \label{t:connessione-per-archi}
	Sia $A$ un insieme aperto e connesso di $\R^n$.
	Per qualsiasi coppia di punti $\vec p_1,\vec p_2\in A$ esiste un arco poligonale con estremi $\vec p_1$ e $\vec p_2$ tutto contenuto in $A$.
\end{teorema}
\begin{proof}
	Sia $\vec p_1\in A$, e chiamiamo $A_1$ l'insieme dei punti in $A$ che sono connessi a $\vec p_1$ per archi (anche poligonali).
	Esso non può essere vuoto, dato che sicuramente contiene almeno un punto, che è $\vec p_1$ stesso.
	Chiamiamo $\mathscr P(\vec q_1,\vec q_2)$ un generico arco che connette i due punti $\vec q_1$ e $\vec q_2$: allora possiamo scrivere $A_1=\{\vec x\in A\colon\exists\mathscr P(\vec x,\vec p_1)\}$.
	Consideriamo un intorno $B(\vec x,\delta)$ e un punto $\vec y$ in esso: il segmento $[\vec x,\vec y]$ che lo congiunge a $\vec x$ è quindi tutto contenuto in tale intorno.
	Tale segmento può essere visto chiaramente come un arco $\mathscr P(\vec x,\vec y)$ che li connette, quindi collegando $\vec x$ a $\vec y$ e $\vec x$ a $\vec p_1$ avremo un arco che connette $\vec p_1$ a $\vec y$; in breve, $\mathscr P(\vec p_1,\vec x)\cup[\vec x,\vec y]=\mathscr P(\vec p_1,\vec y)$.
	Poich\'e questo ragionamento vale per ogni scelta di $\vec y\in B(\vec x,\delta)$, questo intorno è tutto contenuto in $A_1$, che quindi risulta aperto.

	Definiamo ora $A_2\defeq A\setminus A_1$.
	Sia $A_2\neq\emptyset$ (altrimenti il teorema è subito dimostrato), e $\vec x\in A_2$: L'insieme $A$ è aperto, quindi esiste un intorno $B(\vec x,\delta)$ che è contenuto in $A$: sia $\vec y$ un punto di questo intorno.
	Come prima, $[\vec x,\vec y]\subset B(\vec y,\delta)$.
	Supponiamo che $\vec y$ sia in $A_1$: risulterebbe allora che esiste un arco che lo connette a $\vec p_1$, e di conseguenza un arco $\mathscr P(\vec p_1,\vec y)\cup[\vec x,\vec y]$ che connette $\vec x$ a $\vec p_1$.
	Però $\vec x\in A_2$, quindi siamo giunti ad una contraddizione poiché $\vec x$ appartiene sia ad $A_1$ che $A_2$ ma per come li abbiamo definiti essi non possono avere punti in comune.
	Dobbiamo quindi eliminare l'ipotesi fatta che $\vec y\in A_1$, ma allora $\vec y\in A_2$, e come prima $\exists B(\vec y,\delta)$ contenuto in $A_2$, e anche $A_2$ è quindi aperto.

	Alla fine abbiamo quindi che $A=A_1\cup A_2$ e $A_1\cap A_2=\emptyset$, dato che sono complementari rispetto ad $A$.
	Questi due sottoinsiemi sono aperti, e $A$ è connesso, quindi proprio per la definizione di insieme connesso $A_2$ deve essere vuoto (avevamo visto che $A_1$ non lo può essere), ma allora $A_1\equiv A$, che quindi è connesso per archi.
\end{proof}
La proprietà di connessione di un insieme ci permette di estendere una conseguenza del teorema di Lagrange, che affermava che su un intervallo le funzioni sono costanti se e solo se hanno (almeno in tutto l'intervallo) derivata nulla, anche a funzioni di più variabili.
Un insieme connesso prende qui il posto dell'intervallo, che pure è anch'esso un insieme connesso ma solo in $\R$.
\begin{corollario}
	Se $A\subseteq\R^n$ è aperto e connesso, allora una funzione $f\colon A\to\R$ differenziabile è costante se e solo se $\grad f(\vec x)=\vec 0$ per ogni $\vec x\in A$.
\end{corollario}
\begin{proof}
	Se $f$ è costante ha tutte le derivate parziali nulle, quindi segue subito che $\grad f=\vec 0$ in ogni punto di $A$.

	Sia $\grad f=\vec 0$: definiamo, per un $\vec a\in A$, l'insieme di livello $A_1\defeq\{\vec x\in A\colon f(\vec x)=f(\vec a)\}$, e $A_2\defeq A\setminus A_1$ (quest'ultimo sarà quindi l'insieme dei punti la cui immagine \emph{non} è $f(\vec a)$).
	Prendiamo un punto qualsiasi $\vec x'\in A_1$ e un suo intorno $\mathcal U(\vec x')$ che sia tutto contenuto in $A$ (un tale intorno esiste, perch\'e $A$ è aperto).
	Per ogni $\vec x\in\mathcal U(\vec x')$, dunque, per il teorema di Lagrange
	\begin{equation*}
		f(\vec x)-f(\vec x')=\inner[\big]{\grad f\big(\vec x'+c(\vec x-\vec x')\big)}{\vec x-\vec x'}
	\end{equation*}
	per $c\in[0,1]$.
	Ma $\mathcal U(\vec x')\subset A$, quindi per tutti questi punti il gradiente è nullo, quindi $f(\vec x)=f(\vec x')$ $\forall\vec x\in\mathcal U(\vec x')$, ossia $\mathcal U(\vec x')\subset A_1$.
	Poich\'e questo vale per ogni $\vec x'\in A_1$, $A_1$ risulta aperto.
	Allo stesso tempo, però, è anche chiuso: infatti se $\tilde{\vec x}$ è un punto di accumulazione per $A_1$, allora esiste una successione $\{\vec x_n\}\subset A_1$ tale che $\vec x_n\to\tilde{\vec x}$, ma per la continuità di $f$ si ottiene che $f(\vec x_n)\to f(\tilde{\vec x})=f(\vec a)$ e dunque $\tilde{\vec x}\in A_1$.
	Allora anche $A_2$ è aperto, poich\'e è il complementare di un insieme chiuso rispetto ad uno aperto.
	Abbiamo trovato quindi che $A_1$ e $A_2$ sono aperti, e per come li abbiamo costruiti $A_1\cap A_2=\emptyset$; ma $A=A_1\cup A_2$ è connesso, quindi per la definizione $A_2$ deve essere vuoto, vale a dire che $\forall \vec x\in A$ si ha $f(\vec x)=f(\vec a)$, cioè $f$ è costante in $A$.
\end{proof}

\section{Forme differenziali lineari}
Se $V$ e $W$ sono due spazi vettoriali sul medesimo campo generico $K$, sappiamo dal corso di Geometria che lo spazio delle applicazioni lineari da $V$ a $W$, che possiamo indicare con $\lin(V,W)$, ha una struttura di spazio vettoriale: la somma di due elementi $T_1,T_2\in\lin(V,W)$ e il prodotto di un elemento $T\in\lin(V,W)$ per uno scalare $a\in K$ sono ancora delle applicazioni lineari in $\lin(V,W)$, definite per $v\in V$ da
\begin{equation*}
	(T_1+T_2)(v)=T_1(v)+T_2(v)\qtext{e} (aT)(v)=aT(v).
\end{equation*}
La struttura di spazio vettoriale di questo spazio segue in modo naturale dalla linearità delle operazioni.
In particolare, tra queste applicazioni troviamo quelle che da $V$ portano in $K$ (anch'esso è uno spazio vettoriale), dette anche \emph{funzionali lineari}, che quindi appartengono a $\lin(V,K)$.
Questo spazio vettoriale è detto \emph{spazio duale} di $V$ e solitamente si indica anche con $V^*$.

Prendiamo lo spazio vettoriale sul campo dei reali in cui abbiamo sempre lavorato, $\R^n$, e di conseguenza il suo duale, che indicheremo con $(\R^n)^*$.
Vogliamo individuare una relazione che ci permetta di scrivere una base del duale a partire da una base di $\R^n$, che per semplicità sarà per noi la base canonica.
Infatti esiste sempre un isomorfismo che lega i due spazi tra loro, però tale isomorfismo non è \emph{canonico}, ossia dipende sempre dalla base che scegliamo per $\R^n$: basti pensare che per trovare la matrice associata ad un'applicazione lineare è fondamentale la scelta della base (tanto che al cambiare della base la matrice non rimane la stessa), e che dopotutto gli elementi del duale non sono che particolari applicazioni lineari.
Siano $\{\vec e_i\}_{i=1}^n$ e $\{\vec w_i\}_{i=1}^n$ rispettivamente le basi di $\R^n$ e $(\R^n)^*$: scelta la prima, possiamo determinare la seconda in modo che\footnote{Il duale $V^*$ ha la stessa dimensione di $V$, quindi anche le due basi avranno lo stesso numero di elementi.}
\begin{equation}
	\vec w_i(\vec e_j)=\delta_{ij}.
	\label{eq:funzionale-base-canonica}
\end{equation}
In particolare, data la base canonica di $\R^n$, la base corrispondente di $(\R^n)^*$ (definita come prima dalla sua azione sulla base di $\R^n$) diventa una base di vettori tali che $\vec w_i(\vec a)=a_i$, per $\vec a=(a_1,\dots,a_n)\in\R^n$, ossia restituisce la $i$-esima coordinata dei vettori di $\R^n$.

Detto ciò, abbiamo già incontrato in precedenza alcuni elementi di $(\R^n)^*$: i differenziali di funzioni (scalari).
Nel capitolo in cui abbiamo trattato della differenziabilità in più dimensioni avevamo visto che data una $f\in A\subseteq\R^n\to\R$ differenziabile, il suo differenziale $\dd f$ si comporta come un funzionale lineare: preso un punto $\vec x_0\in A$, restituisce un numero reale che è l'incremento di $f$ dal punto $\vec x_0$ relativo all'incremento $\vec h$ della variabile.
Sappiamo inoltre che questo differenziale si può esprimere con il gradiente di $f$, cioè $\dd f(\vec x_0)=\inner{\grad f(\vec x_0)}{\dd\vec x}$, dove abbiamo scritto $\dd\vec x=(\dd x_1,\dots,\dd x_n)$, ossia
\begin{equation}
	\dd f(\vec x_0)=\drp{f}{x_1}(\vec x_0)\,\dd x_1+\drp{f}{x_2}(\vec x_0)\,\dd x_2+\dots+\drp{f}{x_n}(\vec x_0)\,\dd x_n.
	\label{eq:differenziale-duale}
\end{equation}
Ma se il primo membro è proprio il differenziale di $f$, cioè l'applicazione $\vec h\mapsto\inner{\grad f(\vec x_0)}{\vec h}$, allora la notazione ci induce a considerare $\dd x_i$ come l'applicazione che porta $\vec h$ nella sua componente $h_i$.
Infatti, guardando alla \eqref{eq:differenziale-duale} come un funzionale lineare da applicare all'incremento $\vec h$, con la regola che $\dd x_i(\vec h)=h_i$ troviamo
\begin{equation}
	\begin{split}
		\dd f(\vec x_0)(\vec h)&=\drp{f}{x_1}(\vec x_0)\,\dd x_1(\vec h)+\dots+\drp{f}{x_n}(\vec x_0)\,\dd x_n(\vec h)=\\
		&=\drp{f}{x_1}(\vec x_0)h_1+\dots+\drp{f}{x_n}(\vec x_0)h_n
	\end{split}
\end{equation}
che è proprio l'espressione dell'incremento di $f$ associato all'incremento $\vec h$ della variabile.
È naturale allora pensare all'insieme $\{\dd x_1,\dots,\dd x_n\}$ come una base per $(\R^n)^*$: con le combinazioni lineari di elementi di questa base possiamo costruire gli elementi dello spazio (in particolare, i differenziali delle funzioni hanno come coefficienti le rispettive derivate parziali).
Abbiamo anche verificato la proprietà dell'equazione \eqref{eq:funzionale-base-canonica}, ossia risulta
\begin{equation}
	\dd x_j(\vec e_k)=\delta_{jk}
\end{equation}
dove $\{\vec e_i\}_{i=1}^n$ è la base canonica di $\R^n$.
La notazione con gli indici $1,2,3,\dots,n$ è ovviamente arbitraria: se ad esempio in $\R^3$ denotiamo le variabili con $x,y,z$, sarà naturale indicare la base del duale $(\R^3)^*$ con $\{\dd x,\dd y,\dd z\}$.

Lasciamo da parte ora quei particolari elementi del duale che sono i differenziali di funzioni, e guardiamo a tutto lo spazio: un suo elemento generico sarà scritto, in termini della base appena introdotta, come combinazione lineare
\begin{equation*}
	\sum_{i=1}^na_i\,\dd x_i.
\end{equation*}
Possiamo ulteriormente pensare i coefficienti $a_i$ come funzioni dipendenti da una variabile $\vec x$ in $\R^n$: otteniamo dunque una funzione che porta punti di un certo insieme $A\subseteq\R^n$ nel funzionale lineare
\begin{equation*}
	\sum_{i=1}^na_i(\vec x)\,\dd x_i.
\end{equation*}
Queste particolari funzioni sono dette \emph{forme differenziali lineari}.

\begin{definizione} \label{d:forma-differenziale-lineare}
	Sia $A\subseteq\R^n$ aperto.
	Una mappa $\omega\colon A\to(\R^n)^*$ che porta $\vec x\in A$ nel funzionale lineare $\omega(\vec x)$ è detta \emph{forma differenziale lineare}.
\end{definizione}
Le forme differenziali lineari sono anche dette 1-forme.
Ogni forma differenziale lineare si può rappresentare come $\omega=\sum_{i=1}^n\omega_i\,\dd x_i$, e questa scrittura è unica poich\'e la scrittura di un elemento di uno spazio vettoriale in termini di una sua base è sempre univoca.
Le funzioni $\omega_i$, che sono funzioni da $A$ a $\R$, si dicono componenti, o coefficienti della forma, ed esse determinano la regolarità della forma differenziale.
\begin{definizione} \label{d:regolarita-1forme}
	Una forma differenziale lineare $\omega\colon A\subseteq\R^n\to(\R^n)^*$ si dice di classe $\cclass[k]$ se tutte le sue componenti lo sono.
\end{definizione}

Un esempio immediato di forma differenziale, come si può intuire dal discorso svolto finora, è il differenziale di una funzione $f\colon\R^n\to\R$: avremo semplicemente $\omega=\dd f$.
Notiamo subito un'importante differenza nelle notazioni: da una parte abbiamo $\dd f$, che è la 1-forma, cioè un'applicazione da $A$ in $(\R^n)^*$, dall'altra abbiamo invece $\dd f(\vec x)$ che è il differenziale della funzione, ossia un elemento di $(\R^n)^*$ (che è poi l'immagine di $\vec x$ attraverso $\omega$).
\section{Integrale di forme differenziali}
Definiamo ora l'integrale di una forma differenziale lineare lungo una curva di $\R^n$.
\begin{definizione} \label{d:integrale-1forma}
	Sia $A\subseteq\R^n$ aperto e $\omega\colon A\to(\R^n)^*$ una forma differenziale lineare continua.
	Sia inoltre $\vphi\colon[a,b]\to A$ una parametrizzazione di una curva regolare a tratti.
	Si definisce l'integrale di $\omega$ lungo $\vphi$ come
	\begin{equation} 
		\int_{\vphi}\omega=\int_a^b\sum_{i=1}^n\omega_i\big(\vphi(t)\big)\phi_i'(t)\dd t.
		\label{eq:integrale-1forma}
	\end{equation}
\end{definizione}
	La curva $\vphi$ ha $n$ componenti perch\'e è a valori in $A$, quindi è lecito sostituirla alla variabile delle componenti della forma.
	È facile vedere che l'integrale in realtà non dipende dalla parametrizzazione scelta per la curva, ma solo dal suo sostegno: introduciamo quindi il concetto di equivalenza e equiorientazione.
\begin{definizione}
	Siano $\vphi\colon[a,b]\to\R^n$ e $\vpsi\colon[c,d]\to\R^n$ due curve regolari a tratti.
	Esse si dicono \emph{equivalenti} se esiste una riparametrizzazione di $\vphi$ che porta in $\vpsi$, ossia una mappa $\tau\colon[a,b]\to[c,d]$ suriettiva e derivabile in ogni punto di $[a,b]$ tale per cui $\vphi=\vpsi\circ\tau$. 
	Si definiscono inoltre \emph{equiorientate} se la mappa $\tau$ ha derivata positiva in ogni punto di $[a,b]$.
\end{definizione}
Poiché $\tau'(t)>0$ per ogni $t\in[a,b]$ segue che $\tau$ è anche iniettiva e dunque biunivoca tra $[a,b]$ e $[c,d]$.
\begin{teorema}
	Sia $\omega=\sum_{i=1}^na_i\,\dd x_i$ una forma differenziale dall'insieme aperto $A\subseteq\R^n$ a $(\R^n)^*$, di classe $\cclass[1]$, e $\vphi\colon[a,b]\to A$ e $\vpsi\colon [c,d]\to A$ due curve regolari a tratti, equivalenti e con sostegno in $A$.
	Se le due curve sono equiorientate, si ha $\int_{\vphi}\omega=\int_{\vpsi}\omega$; se invece sono orientate in senso opposto, allora $\int_{\vphi}\omega=-\int_{\vpsi}\omega$.
\end{teorema}
\begin{proof}
	Sia $\vphi=\vpsi\circ\tau$ in modo che le due curve siano equivalenti.
	Se sono anche equiorientate, allora $\tau'(t)>0$ $\forall t\in[a,b]$ e $\tau(a)=c$ e $\tau(b)=d$.
	Dunque risulta che
	\begin{equation}
		\int_{\vphi}\omega=\int_a^b\sum_{i=1}^na_i\big(\vphi(t)\big)\phi_i'(t)\,\dd t
		=\int_a^b\sum_{i=1}^na_i\big(\vpsi(\tau(t))\big)\psi_i'\big(\tau(t)\big)\tau'(t)\,\dd t,
	\end{equation}
	calcolando la derivata della funzione composta $\vphi=\vpsi\circ\tau$.
	Chiamiamo allora $r\defeq\tau(t)$ e sostituiamo le variabili nell'integrale: otteniamo che $\tau'(t)\,\dd t=\dd r$, da cui risulta
	\begin{equation}
		\int_{\vphi}\omega=\int_{\tau(a)}^{\tau(b)}\sum_{i=1}^na_i\big(\vpsi(r)\big)\psi_i'(r)\,\dd r
		=\int_c^d\sum_{i=1}^na_i\big(\vpsi(r)\big)\psi_i'(r)\,\dd r=\int_{\vpsi}\omega.
	\end{equation}
	
	Se le due curve hanno invece verso opposto, si ha $\tau'(t)<0$ e dunque la mappa $\tau$ è decrescente, e di conseguenza\footnote{Si ricordi che $\tau\colon[a,b]\to[c,d]$ e di conseguenza deve sempre risultare, almeno, che $a<b$ e $c<d$.} $\tau(a)=d$ e $\tau(b)=c$. Basta ripetere i passaggi compiuti in precedenza, per trovarsi alla fine con gli estremi di integrazione invertiti rispetto a prima, da cui si ricava facilmente che
	\begin{equation*}
		\int_{\vphi}\omega=-\int_{\vpsi}\omega.\qedhere
	\end{equation*}
\end{proof}

\section{Forme differenziali esatte}
Quando abbiamo introdotto le forme differenziali lineari, eravamo partiti dai differenziali di funzioni scalari, che possiamo vedere come particolari forme differenziali i cui coefficienti sono le derivate parziali di una (stessa) funzione:
\begin{equation}
	\dd f=\sum_{i=1}^n\drp{f}{x_i}\dd x_i.
\end{equation}
Gli integrali di queste forme sono semplicissimi da calcolare: infatti dati $A$ aperto di $\R^n$, $f\colon A\to\R$ di classe $\cont[1]{A}$ e $\vphi\colon[a,b]\to\R^n$ regolare a tratti, risulta
\begin{equation}
	\int_{\vphi}\dd f=\int_a^b\sum_{i=1}^n\drp{f}{x_i}\big(\vphi(t)\big)\phi_i'(t)\,\dd t=\int_a^b\drv{}{t}f\big(\vphi(t)\big)\,\dd t.
\end{equation}
Per il teorema fondamentale del calcolo integrale quest'ultimo è uguale a $f\big(\vphi(b)\big)-f\big(\vphi(a)\big)$.
Notiamo che il risultato non cambierebbe se prendessimo un'altra curva con gli stessi estremi.
I differenziali di funzioni sono quindi delle forme differenziali lineari con l'importante proprietà che \emph{il loro integrale non dipende dal cammino scelto, ma solo dagli estremi}.
Questo semplifica di molto il calcolo di questi integrali, perciò vogliamo capire quando possiamo assicurarci che una forma sia di questo tipo.
In una sola dimensione tutte le forme differenziali lineari continue sono anche dei differenziali di funzioni, ma questo non è più vero in più dimensioni.
\begin{definizione} \label{d:forma-differenziale-esatta}
	Sia $A\subseteq\R^n$ aperto e $\omega\colon A\to(\R^n)^*$ una forma differenziale lineare continua in $A$.
	Essa si dice \emph{esatta} se esiste una funzione (detta \emph{potenziale}) $F\colon A\to\R$ di classe $\cont[1]{A}$ tale per cui $\dd F=\omega$.
\end{definizione}
\begin{osservazione}
	Se $F$ è un potenziale di $\omega$ su $A$, allora anche $G=F+c$, dove $c$ è una qualsiasi costante, lo è.
	D'altra parte, se $F_1$ e $F_2$ sono entrambi potenziali di una medesima forma differenziale su $A$, e $A$ è connesso, allora differiscono di una costante. %Infatti $\dd(F_1-F_2)=0$ da cui\dots
	Una forma differenziale esatta ammette quindi infiniti potenziali, ma tutti sono uguali a meno di una costante additiva (si può dunque determinare univocamente un potenziale a meno di tale costante).
\end{osservazione}
Introduciamo ora due classi di curve: la classe delle curve regolari a tratti definite da uno stesso intervallo e con estremi dati nell'insieme $A$, ossia l'insieme $\Phi_{\vec a,\vec b}(A)\defeq\{\vphi\colon[\alpha,\beta]\to A\text{ regolari a tratti e con }\vphi(\alpha)=\vec a, \vphi(\beta)=\vec b\}$, e la classe delle curve chiuse $\Phi_c(A)\defeq\{\vphi\colon[\alpha,\beta]\to A\text{ regolare a tratti e per cui }\vphi(\alpha)=\vphi(\beta)\}$, cioè con estremi coincidenti, in $A$.
Sfruttiamo questa definizione per enunciare i seguenti teoremi.
\begin{teorema}
	Sia $A\subseteq\R^n$ aperto e connesso, $\omega\colon A\to(\R^n)^*$ una forma differenziale lineare continua.
	Essa è esatta se e solo se $\int_{\vphi}\omega=0$ per ogni $\vphi\in\Phi_c(A)$.
\end{teorema}
\begin{teorema}
	Sia $A\subseteq\R^n$ aperto e connesso e $\omega\colon A\to(\R^n)^*$ continua.
	Essa è una forma differenziale esatta se e solo se $\int_{\vphi}\omega=\int_{\vpsi}\omega$ per qualsiasi coppia $\vphi,\vpsi$ nella classe $\Phi_{\vec a,\vec b}(A)$ e per ogni scelta di $\vec a$ e $\vec b$ in $A$.
\end{teorema}
\begin{proof}
	Se $\omega$ è esatta, segue dalla definizione \ref{d:forma-differenziale-esatta} che esiste una funzione $f$ tale per cui $\dd f=\omega$, e allora risulta $\int_{\vphi}\dd f=f\big(\vphi(\beta)\big)-f\big(\vphi(\alpha)\big)$.
	Una curva $\vpsi\in\Phi_{\vec a,\vec b}(A)$ ha gli stessi estremi, siano essi $\vpsi(\gamma)=\vphi(\alpha)$ e $\vpsi(\delta)=\vphi(\beta)$, e dunque
	\begin{equation}
		\int_{\vpsi}\dd f=f\big(\vpsi(\delta)\big)-f\big(\vpsi(\gamma)\big)=f\big(\vphi(\beta)\big)-f\big(\vphi(\alpha)\big)=\int_{\vphi}\dd f.
	\end{equation}
	
	Sia ora $\vec p\in A$: poiché l'insieme è connesso per archi, per ciascun punto $\vec x\in A$ esiste una curva $\vphi$ regolare a tratti che connette i due punti, vale a dire $\vphi\in\Phi_{\vec p,\vec x}(A)$.
	Sia allora $F(\vec x)=\int_{\vphi}\omega$: per ipotesi, l'integrale dipende soltanto dagli estremi della curva, quindi fissato $\vec p$ la $F$ è ben definita.
	L'insieme $A$ è aperto, quindi esiste sempre un intorno $B(\vec x,\delta)\subset A$ per ogni $\vec x\in A$.
	Vogliamo ora calcolare il rapporto incrementale di $F$ lungo una direzione arbitraria, ai fini di calcolare la derivata direzionale, e vedremo che i coefficienti $a_i$ della forma sono le derivate parziali di $F$.
	Prendiamo un versore $\vec v\in\R^n$, e per $h\in\R$ valutiamo l'incremento $\vec q=\vec x+h\vec v$.
	Per $\abs{h}$ sufficientemente piccolo, anche $\vec q$ è in $A$.
	Chiamiamo allora $\vpsi$ la curva
	\begin{equation*}
		\vpsi(t)=
		\begin{cases}
			\vphi(t)	&t\in[\alpha,\beta]\\
			\vec x+(t-\beta)h\vec v	&t\in(\beta,\beta+1]
		\end{cases}
	\end{equation*}
	Essa va da $\vpsi(\alpha)=\vec p$ a $\vpsi(\beta+1)=\vec x+(\beta+1-\beta)h\vec v=\vec q$, passando per $\vpsi(\beta)=\vec x$, ed è regolare a tratti con sostegno in $A$.
	Calcoliamo dunque l'incremento di $F$, ossia $F(\vec x+h\vec v)-F(\vec x)=\int_{\vpsi}\omega-\int_{\vphi}\omega$, che per definizione è
	\begin{equation}
		\int_\alpha^{\beta+1}\sum_{i=1}^na_i\big(\vpsi(t)\big)\psi_i'(t)\,\dd t-\int_\alpha^\beta\sum_{i=1}^na_i\big(\vphi(t)\big)\phi_i'(t)\,\dd t.
	\end{equation}
	Nell'intervallo $[\alpha,\beta]$, però, le due curve $\vphi$ e $\vpsi$ coincidono, quindi $a_i\big(\vphi(t)\big)=a_i\big(\vpsi(t)\big)$ per ogni $t$ in tale intervallo e per ogni $i=1,\dots,n$.
	Allora l'incremento di $F$ diventa
	\begin{equation}
		\begin{aligned}
			&\int_\alpha^\beta\sum_{i=1}^na_i\big(\vphi(t)\big)\phi_i'(t)\,\dd t+\int_\beta^{\beta+1}\sum_{i=1}^na_i\big(\vpsi(t)\big)\psi_i'(t)\,\dd t-\int_\alpha^\beta\sum_{i=1}^na_i\big(\vphi(t)\big)\phi_i'(t)\,\dd t=\\
			=&\int_\beta^{\beta+1}\sum_{i=1}^na_i\big(\vpsi(t)\big)\psi_i'(t)\,\dd t=\\
			=&\int_\beta^{\beta+1}\sum_{i=1}^na_i\big(\vec x+(t-\beta)h\vec v)hv_i\,\dd t.
		\end{aligned}
	\end{equation}
	Effettuiamo ora un cambio di variabile, ponendo $r(t)=h(t-\beta)$, per cui si avrà $r(\beta)=0$, $r(\beta+1)=h$ e $\dd r=h\,\dd t$: l'integrale precedente diventa
	\begin{equation}
		\int_0^h\sum_{i=1}^na_i\big(\vec x+r\vec v)v_i\,\dd r.
	\end{equation}
	Dividendo per $h$ e passando al limite per $h\to0$, anche $r$ tende a zero, quindi possiamo passare dal limite per $h$ al limite per $r$.
	Dal teorema fondamentale del calcolo integrale otteniamo la derivata di $F$ in $\vec x$ lungo la direzione indicata da $\vec v$:
	\begin{equation}
		\lim_{h\to0}\frac{F(\vec x+h\vec v)-F(\vec x)}{h}=\lim_{h\to0}\frac1{h}\int_0^h\sum_{i=1}^na_i(\vec x+r\vec v)v_i\,\dd r=\lim_{r\to 0}\sum_{i=1}^na_i(\vec x+r\vec v)v_i=\sum_{i=1}^na_i(\vec x)v_i.
	\end{equation}
	Possiamo vederlo anche con il teorema di De L'H\^opital, derivando rispetto ad $h$ poich\'e numeratore e denominatore tendono entrambi a zero: la derivata del denominatore $h$ è chiaramente 1, quella del numeratore, che è una funzione integrale, per il teorema fondamentale del calcolo integrale è invece è la funzione integranda valutata in $h$ (a questo punto non sarebbe nemmeno necessario passare al limite in $r$).
	In ogni caso, notiamo che, poiché la derivata direzionale si può ottenere come prodotto scalare tra il gradiente di $F$ e $\vec v$, è proprio $\grad F=(a_1,\dots,a_n)$.
	Allora si ottiene
	\begin{equation}
		\omega=\sum_{i=1}^na_i\dd x_i=\inner{\grad F}{\dd\vec x}=\dd F,
	\end{equation}
	ossia $\omega$ è esatta.
\end{proof}

\section{Forme differenziali chiuse}
Le forme esatte sono importanti, ma i teoremi che abbiamo visto finora sono poco utili a individuarle: è chiaramente impossibile verificare che l'integrale sia nullo lungo \emph{ogni} curva chiusa.
Dobbiamo quindi cercare delle condizioni più facili da verificare, e sufficienti ad affermare che una forma è esatta.

Se una forma differenziale è esatta in un insieme $A$ connesso e aperto ed è di classe $\cont[1]{A}$, allora il suo potenziale è una funzione $F\in\cont[2]{A}$: per essa vale il teorema di Schwarz, per cui le derivate miste coincidono.
Poiché la forma è esatta, $\drp{F}{x_i}(\vec x)=a_i(\vec x)$, perciò
\begin{equation}
	\drp{a_i}{x_j}(\vec x)=\frac{\partial^2 F}{\partial x_j\partial x_i}(\vec x)=\frac{\partial^2 F}{\partial x_i\partial x_j}(\vec x)=\drp{a_j}{x_i}(\vec x).
\end{equation}
Questa proprietà che le derivate miste ``incrociate'' sono uguali si ritrova nella seguente definizione.
\begin{definizione} \label{d:forma-diff-chiusa}
	Una forma differenziale lineare $\omega\in\cont[1]{A}$ per un insieme $A\in\R^n$ aperto si dice \emph{chiusa} in tale insieme se
	\begin{equation}
		\drp{a_i}{x_j}(\vec x)=\drp{a_j}{x_i}(\vec x)
	\end{equation}
	per ogni $\vec x\in A$ e $i,j\in\{1,\dots,n\}$ con $i\neq j$.
\end{definizione}
Da quanto appena detto è evidente che tutte le forme differenziali esatte sono anche chiuse, ma questa proprietà si ritrova anche in altre forme, per cui non vale l'inverso.
\begin{esempio} \label{es:forma-differenziale-angolo-R2}
	Consideriamo la forma differenziale
	\begin{equation*}
		\omega(x,y)=a_1(x,y)\,\dd x+a_2(x,y)\,\dd y=\frac{y}{x^2+y^2}\,\dd x-\frac{x}{x^2+y^2}\,\dd y
	\end{equation*}
	in $A=\R^2\setminus\{\vec 0\}$.
	Essa è $\cont[\infty]{A}$, ed è chiusa, infatti
	\begin{equation}
		\drp{}{y}\frac{y}{x^2+y^2}=\frac{x^2-y^2}{(x^2+y^2)^2}=\drp{}{x}\frac{-x}{x^2+y^2}
	\end{equation}
	Notiamo inoltre che la funzione $F(x,y)=\arctan\frac{x}{y}$, a meno di costanti arbitrarie, è tale per cui $\drp{F}{x}(x,y)=a_1(x,y)$ e $\drp{F}{y}(x,y)=a_2(x,y)$ per ogni $(x,y)\in A$, quindi se esiste un potenziale non può che essere della forma $F(x,y)+c$ con $c\in\R$.
	A causa della singolarità per $y=0$ dobbiamo però prima dividere $A$ nei due sottoinsiemi per $y<0$ e $y>0$, in cui avremo rispettivamente $F(x,y)+c$ e $F(x,y)+d$ (a priori non possiamo dire che sono uguali).
	Determiniamo queste costanti: fissato $x_0<0$, abbiamo i limiti
	\begin{equation}
		\lim_{\substack{x\to x_0\\ y\to 0^+}}F(x,y)=c-\frac{\pi}2\qtext{e}\lim_{\substack{x\to x_0\\ y\to 0^-}}F(x,y)=d+\frac{\pi}2,
	\end{equation}
	che devono coincidere, quindi $c=d+\pi$ e abbiamo la funzione
	\begin{equation}
		F(x,y)=
		\begin{cases}
			\arctan\frac{x}{y}+c	&y>0\\
			c-\frac{\pi}2			&y=0\\
			\arctan\frac{x}{y}+c-\pi	&y<0
		\end{cases}
	\end{equation}
	Però se prendiamo $x_1>0$, abbiamo i limiti
	\begin{equation}
		\lim_{\substack{x\to x_1\\ y\to 0^+}}F(x,y)=\frac{\pi}2+c\qtext{e}\lim_{\substack{x\to x_1\\ y\to 0^-}}F(x,y)=-\frac{3\pi}2+c,
	\end{equation}
	ma allora $F$ è discontinua in $A$, perciò non può essere il potenziale di $\omega$, che quindi non ammette potenziali e non è esatta.
\end{esempio}
Il problema da superare non risiede tanto nella funzione, ma nell'insieme in cui la stiamo studiando: è ovvio che se al posto di $A$ avessimo preso solo, ad esempio, il suo sottoinsieme in cui $x>0$, non ci sarebbero stati problemi di continuità e avremmo senza problemi trovato un potenziale per $\omega$.
Dobbiamo quindi restringere la classe degli insiemi ``buoni'' che ci permettano di dire che quando una forma è chiusa è anche esatta: tali insiemi sono gli insiemi \emph{semplicemente connessi}, che intuitivamente si possono pensare come insiemi ``senza buchi''.
Una definizione rigorosa può essere data tramite il concetto di curve omotopiche.
\begin{definizione} \label{d:curva-omotopica}
	Siano $\vphi_1,\vphi_2\colon[a,b]\to A\subset\R^n$ due parametrizzazioni di curve chiuse e $\cont[2]{[a,b]}$.
	La curva $\vphi_1$ si dice \emph{omotopica} a $\vphi_2$ se esiste una mappa (detta appunto \emph{omotopia}) $\vxi\colon[0,1]\times[a,b]\to A$, di classe $\cclass[2]$, tale per cui $\vxi(0,t)=\vphi_1(t)$ e $\vxi(1,t)=\vphi_2(t)$, e per ogni $s\in[0,1]$ si abbia $\vxi(s,a)=\vxi(s,b)$.
\end{definizione}
La mappa dell'omotopia tra le due curve si può vedere più facilmente come una deformazione continua (quindi ``senza strappi'') e differenziabile che trasforma curve chiuse in curve chiuse nello stesso insieme.
\begin{definizione} \label{d:insieme-semplicemente-connesso}
	Un insieme $A\subseteq\R^n$ si dice \emph{semplicemente connesso} se ogni curva $\vphi\colon[a,b]\to A$, di classe $\cont[2]{[a,b]}$ e chiusa è omotopica ad un punto in $A$, ossia ad una curva (degenere) costante $\vpsi\colon[a,b]\to A$ per cui $\vpsi(t)=\vec c$, con $\vec c\in A$.
\end{definizione}
Con queste definizioni possiamo quindi enunciare il seguente importante teorema, noto principalmente come \emph{lemma di Poincar\'e}.
\begin{teorema}[Poincar\'e] \label{t:poincare}
	Sia $A\subseteq\R^n$ un insieme aperto e semplicemente connesso, e $\omega$ una forma differenziale lineare di classe $\cont[1]{A}$.
	Se $\omega$ è chiusa in $A$, allora è anche esatta in tale insieme.
\end{teorema}
La forma vista nell'esempio precedente può non essere esatta in $\R^2\setminus\{\vec 0\}$ perch\'e l'insieme non è semplicemente connesso: ogni curva che circonda l'origine non può essere deformata in un punto (dell'insieme) tramite un'omotopia, perch\'e abbiamo il ``buco'' nell'origine che lo impedisce.
Questo risultato è molto importante, perch\'e fornisce le condizioni meno restrittive per affermare l'esattezza della forma.
La sua dimostrazione però è piuttosto complicata, e non la trattiamo: possiamo invece verificarne la validità su insiemi più semplici, come gli insiemi cosiddetti \emph{stellati} o gli insiemi convessi, che sono tutti semplicemente connessi quindi per le forme in tali insiemi vale il lemma di Poincar\'e.
\begin{definizione} \label{d:insieme-stellato}
	Un insieme $A\subseteq\R^n$ aperto si dice \emph{stellato} rispetto ad un suo punto $\vec x_0$ se per ogni $\vec x\in A$ si ha $[\vec x,\vec x_0]\subset A$, cioè il segmento che li unisce è tutto contenuto in $A$.
\end{definizione}
Alcuni semplici insiemi stellati sono gli insiemi convessi, per i quali il segmento che unisce due punti \emph{qualsiasi} dell'insieme è interno ad esso, dunque sono stellati rispetto a tutti i loro punti.
Vediamo quindi una dimostrazione del lemma di Poincar\'e per questi insiemi stellati.
\begin{teorema} \label{t:poincare-stellato}
	Sia $A\subseteq\R^n$ un insieme aperto e stellato, e $\omega$ una forma differenziale lineare in $A$.
	Se $\omega$ è chiusa in $A$, allora è anche esatta in tale insieme.
\end{teorema}
\begin{proof} Prendiamo un punto $\vec x_0\in A$ rispetto al quale l'insieme è stellato, e per ogni $\vec x\in A$ parametrizziamo il segmento $[\vec x_0,\vec x]$ con la funzione $\vphi(t)=\vec x_0+(\vec x-\vec x_0)t$, con $t\in[0,1]$: chiaramente è una curva con sostegno in $A$.
	Per mostrare l'esattezza della forma, definiamo $F(\vec x)\defeq\int_{\vphi}\omega$ e verifichiamo che ne è il potenziale.
	Calcolando l'integrale della forma lungo il segmento dato otteniamo
	\begin{equation}
		F(\vec x)=\int_0^1\sum_{i=1}^na_i\big(\vec x_0+(\vec x-\vec x_0)t\big)(x_i-x_{0,i})\,\dd t.
	\end{equation}
	Si ha che $\omega$ è almeno di classe $\cont[1]{A}$, in quanto chiusa, quindi per la sua continuità e per la linearità della derivazione risulta
	\begin{equation}
		\begin{split}
			\drp{F}{x_j}(\vec x)&=\drp{}{x_j}\int_0^1\sum_{i=1}^na_i\big(\vphi(t)\big)(x_i-x_{0,i})\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n\drp{}{x_j}\Big[a_i\big(\vphi(t)\big)(x_i-x_{0,i})\Big]\,\dd t.
		\end{split}
	\end{equation}
	Svolgendo la derivata del prodotto, troviamo
	\begin{equation}
		\begin{split}
			\drp{F}{x_j}(\vec x)&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\drp{a_i}{x_j}\big(\vphi(t)\big)\,\dd t+\int_0^1\sum_{i=1}^na_i\big(\vphi(t)\big)\drp{}{x_j}(x_i-x_{0,i})\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\drp{a_i}{x_j}\big(\vphi(t)\big)\,\dd t+\int_0^1\sum_{i=1}^na_i\big(\vphi(t)\big)\delta_{ij}\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\drp{a_i}{x_j}\big(\vphi(t)\big)\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t.
		\end{split}
	\end{equation}
	Possiamo dunque scambiare $\drp{a_i}{x_j}$ con $\drp{a_j}{x_i}$ in virtù della chiusura della forma.
	Inoltre, sviluppiamo la derivata di $a_j$ nel prodotto delle derivate delle due funzioni composte, $a_j$ e $\vphi$:
	\begin{equation}
		\begin{split}
			\drp{F}{x_j}(\vec x)&=\int_0^1\sum_{i=1}^n\drp{a_j}{x_i}\big(\vphi(t)\big)(x_i-x_{0,i})\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\grad a_j\big(\vphi(t)\big)\drp{\vphi}{x_i}(t)\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\sum_{k=1}^n\drp{a_j}{x_k}\big(\vphi(t)\big)\drp{\phi_k}{x_i}(t)\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\sum_{k=1}^n\drp{a_j}{x_k}\big(\vphi(t)\big)\drp{}{x_i}\big(t(x_k-x_{0,k})\big)\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\sum_{k=1}^n\drp{a_j}{x_k}\big(\vphi(t)\big)t\delta_{ik}\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t=\\
			&=\int_0^1\sum_{i=1}^n(x_i-x_{0,i})\drp{a_j}{x_i}\big(\vphi(t)\big)t\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t.
		\end{split}
	\end{equation}
	Riconosciamo ora nel termine della sommatoria, a meno del fattore $t$, la derivata totale rispetto al tempo della funzione $a_j\big(\vphi(t)\big)$, per cui
	\begin{equation}
		\drp{F}{x_j}(\vec x)=\int_0^1t\drv{a_j}{t}\big(\vphi(t)\big)\,\dd t+\int_0^1a_j\big(\vphi(t)\big)\,\dd t,
	\end{equation}
	e riunendo gli integrali abbiamo la derivata temporale del prodotto $ta_j\big(\vphi(t)\big)$, quindi
	\begin{equation}
		\drp{F}{x_j}(\vec x)=\int_0^1\drv{}{t}\Big[ta_j\big(\vphi(t)\big)\Big]\,\dd t=ta_j\big(\vec x_0+(\vec x-\vec x_0)t\big)\bigg|_0^1=a_j(\vec x),
	\end{equation}
	ma allora $F$ è il potenziale di $\omega$, che dunque è esatta.
\end{proof}
La dimostrazione appena eseguita indica anche un metodo per individuare, se esiste, il potenziale di una forma su un insieme stellato: si prende un punto nell'insieme dato (rispetto al quale l'insieme è stellato!), e si calcola l'integrale della forma lungo un segmento che connette tale punto ad un generico $\vec x$, sempre nell'insieme.
\begin{esempio} \label{es:calcolo-potenziale}
	Consideriamo in $A=\{(x,y)\in\R^2\colon y>x\}$ la forma differenziale
	\begin{equation*}
		\omega(x,y)=a_1(x,y)\,\dd x+a_2(x,y)\,\dd y=\frac{2xy-2x^2-1}{y-x}\,\dd x+\frac{y-x+1}{y-x}\,\dd y.
	\end{equation*}
	Essa è di classe $\cont[\infty]{A}$, ed è chiusa: infatti
	\begin{equation}
		\drp{a_1}{y}(x,y)=\drp{}{y}\bigg(2x-\frac1{y-x}\bigg)=\frac1{(y-x)^2}=\drp{a_2}{x}(x,y).
	\end{equation}
	L'insieme $A$ è convesso, quindi $\omega$ è anche esatta.
	Prendiamo il punto $(0,1)$ e la curva $\vphi(t)=\big(tx,1+(y-1)t\big)$ che lo connette con un generico $(x,y)\in A$, con $t\in[0,1]$.
	Il suo potenziale è la funzione $F\colon A\to\R$ determinata da
	\begin{equation}
		\begin{split}
			F(\vec x)&=\int_{\vphi}\omega=\int_0^1\bigg[2tx-\frac1{1+(y-1)t-tx}\bigg]x\,\dd t+\int_0^1\bigg[1+\frac1{1+(y-1)t-tx}\bigg](y-1)\,\dd t=\\
			&=\int_0^12tx^2\,\dd t+\int_0^1(y-1)\,\dd t+\int_0^1\frac{y-x-1}{1+(y-1)t-t}\,\dd t=\\
			&=x^2\int_0^12t\,\dd t+(y-1)\int_0^1\dd t+\int_0^1\frac{y-x-1}{1+(y-x-1)t}\,\dd t=\\
			&=x^2+y-1+\log\big[1+(y-x-1)t\big]\bigg|_0^1=x^2+y-1+\log(y-x).
		\end{split}
	\end{equation}
	Rigorosamente, questa funzione è solo una dei potenziali di $\omega$, che sono della forma $F+c$ per qualsiasi $c\in\R$.
\end{esempio}
La scelta di calcolare l'integrale dal punto $(0,1)$ ci ha dato un potenziale che si annulla in tale punto.
Questo è utile quando è espressamente richiesto che il potenziale soddisfi una condizione di questo tipo: se vogliamo che $F$ valga $F_0\in\R$ in un dato punto $\vec x'$, troveremo subito il potenziale richiesto con la formula
\begin{equation}
	F(\vec x)=F_0+\int_{[\vec x',\vec x]}\omega
\end{equation}
assicurandoci chiaramente che l'insieme sia stellato rispetto a $\vec x'$.
